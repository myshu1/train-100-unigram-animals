# ============================================================
# CITATION.cff (How to cite this project)
# ============================================================

cff-version: "1.2.0"
type: software

title: "Toy-GPT: Unigram Next-Token Language Model with Structured Corpus"
version: "0.9.5"
date-released: "2026-01-18"

authors:
  - family-names: Case
    given-names: Denise M.
    orcid: "https://orcid.org/0000-0001-6165-7389"
    affiliation: "Northwest Missouri State University, School of Computer Science and Information Systems, Maryville, MO, USA"

repository-code: "https://github.com/toy-gpt/train-100-unigram-animals"
url: "https://github.com/toy-gpt/train-100-unigram-animals"
license: MIT

abstract: >
  Inspectable language-model training repositories demonstrating
  next-token prediction across controlled corpus regimes.
  This repository implements a unigram next-token prediction model trained with
  softmax and cross-entropy loss on a small structured corpus.
  It assists with understanding probabilistic language modeling,
  training loops, and inspectable model artifacts
  prior to introducing higher-order context, embeddings, or attention mechanisms.

keywords:
  # domain
  - language-models
  - natural-language-processing
  - probabilistic-models
  - next-token-prediction

  # pedagogy
  - education
  - computer-science-education
  - machine-learning-education
  - teaching
  - learning

  # methodology
  - unigram-model
  - softmax
  - cross-entropy-loss
  - gradient-descent
  - structured-corpus

  # software practice
  - python
  - reproducible-research
  - inspectable-artifacts
  - open-source
  - github

message: "If you use this project, please cite it using this file."
